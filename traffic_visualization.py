import h5py
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from scipy.ndimage import binary_erosion, binary_dilation, median_filter
from skimage.transform import hough_line, hough_line_peaks
import json
import os
import time
import logging
from datetime import datetime
from io import BytesIO
import base64
import seaborn as sns
from matplotlib.lines import Line2D
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
TIME_STEP = 0.62
DISTANCE_METERS = 4000
FREQ_CHANNELS = 96
MIN_TRACK_LENGTH = 8  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç—Ä–µ–∫–æ–≤
MIN_SPEED_KMH = 5
MAX_SPEED_KMH = 120
CONGESTION_SPEED_THRESHOLD = 25

# –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤
FREQ_WEIGHTS = np.ones(FREQ_CHANNELS, dtype=np.float32)
FREQ_WEIGHTS[:20] = 1.6    # –ù–∏–∑–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è –≥—Ä—É–∑–æ–≤—ã—Ö
FREQ_WEIGHTS[20:40] = 1.4  # –°—Ä–µ–¥–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è –ª–µ–≥–∫–æ–≤—ã—Ö
FREQ_WEIGHTS[40:] = 0.4    # –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ —à—É–º–æ–≤

class TrafficVisualization:
    def __init__(self, combined_data_path, output_dir):
        self.combined_data_path = combined_data_path
        self.output_dir = output_dir
        self.ensure_directories()
        self.start_time = time.time()
        self.processed_data = None
        self.timestamps = None
    
    def ensure_directories(self):
        if self.output_dir:
            os.makedirs(self.output_dir, exist_ok=True)
    
    def log_time(self, message):
        elapsed = time.time() - self.start_time
        logger.info(f"{message} | –í—Ä–µ–º—è: {elapsed:.2f} —Å–µ–∫")
        self.start_time = time.time()

    def load_combined_data(self):
        """–ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        if not os.path.exists(self.combined_data_path):
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.combined_data_path}")
        
        with h5py.File(self.combined_data_path, 'r') as f:
            stats = f['statistics'][:]
            timestamps = f['timestamps'][:]
            
            if timestamps.ndim == 2 and timestamps.shape[1] == 3:
                timestamps = timestamps[:, 0] + timestamps[:, 1] / 1000.0
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {stats.shape}")
        return stats, timestamps

    def optimized_preprocess(self, stats):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"""
        logger.info("üîß –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞...")
        
        # 1. –ë—ã—Å—Ç—Ä–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ (—Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ)
        stats_float = stats.astype(np.float32)
        data_2d = np.zeros((stats.shape[0], stats.shape[1]), dtype=np.float32)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 50 –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        for freq_idx in range(min(50, stats.shape[2])):
            data_2d += stats_float[:, :, freq_idx] * FREQ_WEIGHTS[freq_idx]
        
        # 2. –ë—ã—Å—Ç—Ä–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        filtered_data = median_filter(data_2d, size=(1, 2))
        
        # 3. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        p70 = np.percentile(filtered_data, 70)
        p85 = np.percentile(filtered_data, 85)
        threshold = p70 + 0.2 * (p85 - p70)
        
        binary = filtered_data > threshold
        
        # 4. –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        binary = binary_erosion(binary, structure=np.ones((1, 2)))
        binary = binary_dilation(binary, structure=np.ones((1, 2)))
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {filtered_data.shape}")
        logger.info(f"–ü–æ—Ä–æ–≥: {threshold:.2f}, p70: {p70:.2f}, p85: {p85:.2f}")
        
        return filtered_data, binary

    def detect_tracks_fast(self, signal_2d, binary_mask):
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç—Ä–µ–∫–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        T, L = signal_2d.shape
        logger.info(f"üîç –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ —Ç—Ä–µ–∫–æ–≤...")
        
        try:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∏–∫–æ–≤ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ—Ö–≤–∞—Ç–∞
            h, theta, d = hough_line(binary_mask)
            _, angles, dists = hough_line_peaks(
                h, theta, d,
                num_peaks=400,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç—Ä–µ–∫–æ–≤
                threshold=0.1 * h.max(),  # –ü–æ–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥
                min_distance=8,   # –£–º–µ–Ω—å—à–µ–Ω–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
                min_angle=3      # –£–º–µ–Ω—å—à–µ–Ω–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø–æ —É–≥–ª—É
            )
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –•–∞—Ñ–∞: {e}")
            return []

        tracks = []
        for angle, dist in zip(angles, dists):
            deg = np.degrees(angle)
            if not (3 < abs(deg) < 87):
                continue
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —Ç–æ—á–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
            t_vals = np.arange(T)
            x_vals = (dist - t_vals * np.sin(angle)) / np.cos(angle)
            valid = (x_vals >= 0) & (x_vals < L)
            
            if np.sum(valid) < MIN_TRACK_LENGTH:
                continue
                
            track_points = list(zip(t_vals[valid], x_vals[valid]))
            
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ –ø–µ—Ä–≤—ã–º 10 —Ç–æ—á–∫–∞–º
            signal_sum = 0
            count = 0
            for t, x in track_points[:10]:
                t_idx, x_idx = int(t), int(x)
                if binary_mask[t_idx, x_idx]:
                    signal_sum += signal_2d[t_idx, x_idx]
                    count += 1
            
            if count >= 3:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–æ—á–µ–∫
                tracks.append(track_points)
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç—Ä–µ–∫–æ–≤: {len(tracks)}")
        return tracks

    def smart_classify_vehicle(self, signal_2d, track_points):
        """–£–º–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        if not track_points:
            return "light", 0.0
        
        # –°–æ–±–∏—Ä–∞–µ–º –∞–º–ø–ª–∏—Ç—É–¥—ã —Å —à–∞–≥–æ–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        amplitudes = []
        step = max(1, len(track_points) // 20)  # –ù–µ –±–æ–ª–µ–µ 20 —Ç–æ—á–µ–∫
        for i in range(0, len(track_points), step):
            t, x = track_points[i]
            t_idx, x_idx = int(t), int(x)
            if (0 <= t_idx < signal_2d.shape[0] and 
                0 <= x_idx < signal_2d.shape[1]):
                amplitudes.append(signal_2d[t_idx, x_idx])
        
        if not amplitudes:
            return "light", 0.0
            
        avg_amp = np.mean(amplitudes)
        max_amp = np.max(amplitudes)
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        data_mean = np.mean(signal_2d)
        data_std = np.std(signal_2d)
        
        # –ö–û–†–†–ï–ö–¢–ù–´–ï –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞—à–∏—Ö –ª–æ–≥–æ–≤
        # –ò–∑ –ª–æ–≥–æ–≤: avg_amp –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0.8-2.0, max_amp 8-30
        # data_mean –æ–∫–æ–ª–æ 0, data_std –æ–∫–æ–ª–æ 1 –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –≤–º–µ—Å—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö
        if avg_amp > 2.5 or max_amp > 25:  # –í—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è - –≥—Ä—É–∑–æ–≤—ã–µ
            return "heavy", float(avg_amp)
        elif avg_amp > 1.0:  # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è - –ª–µ–≥–∫–æ–≤—ã–µ
            return "light", float(avg_amp)
        else:  # –ù–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è - —Ç–æ–∂–µ –ª–µ–≥–∫–æ–≤—ã–µ
            return "light", float(avg_amp)

    def fast_generate_analysis(self):
        """
        –ë–´–°–¢–†–´–ô –ò –≠–§–§–ï–ö–¢–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó
        """
        try:
            total_start = time.time()
            
            # 1. –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
            stats, timestamps = self.load_combined_data()
            
            # 2. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
            data_2d, binary = self.optimized_preprocess(stats)
            
            # 3. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç—Ä–µ–∫–æ–≤
            raw_tracks = self.detect_tracks_fast(data_2d, binary)
            
            # 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç—Ä–µ–∫–æ–≤
            tracks = []
            for i, track_points in enumerate(raw_tracks):
                if len(track_points) < MIN_TRACK_LENGTH:
                    continue
                    
                # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–µ–∫ —Ç—Ä–µ–∫–∞ (–∫–∞–∂–¥–∞—è —Ç–æ—á–∫–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)
                track_data = []
                for t, x in track_points:
                    time_val = timestamps[int(t)] if int(t) < len(timestamps) else timestamps[-1]
                    track_data.append({
                        'time': float(time_val),
                        'position': float(x)
                    })
                
                # –£–º–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                vehicle_type, avg_amp = self.smart_classify_vehicle(data_2d, track_points)
                
                # –†–∞—Å—á–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏
                speed_kmh = 0
                if len(track_data) > 1:
                    dt = track_data[-1]['time'] - track_data[0]['time']
                    dx = track_data[-1]['position'] - track_data[0]['position']
                    if dt > 0:
                        speed_kmh = abs(dx / dt * 3.6)
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏
                if MIN_SPEED_KMH <= speed_kmh <= MAX_SPEED_KMH:
                    tracks.append({
                        'id': i,
                        'points': track_data,
                        'vehicle_type': vehicle_type,
                        'avg_amp': avg_amp,
                        'speed_kmh': speed_kmh
                    })
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            light_count = sum(1 for t in tracks if t['vehicle_type'] == 'light')
            heavy_count = sum(1 for t in tracks if t['vehicle_type'] == 'heavy')
            logger.info(f"üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï: –ª–µ–≥–∫–æ–≤—ã–µ={light_count}, –≥—Ä—É–∑–æ–≤—ã–µ={heavy_count}")
            
            # 5. –ë—ã—Å—Ç—Ä–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats_result = self.create_fast_statistics(tracks, timestamps)
            
            # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self.save_results_fast(tracks, stats_result)
            
            # 7. –ë—ã—Å—Ç—Ä—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            visualization_results = self.create_fast_visualizations(tracks, data_2d, timestamps, stats_result)
            
            total_time = time.time() - total_start
            logger.info(f"üéâ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù! –í—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫")
            
            return {
                "success": True,
                "tracks_count": len(tracks),
                "statistics": stats_result,
                "processing_time": total_time,
                "visualizations": visualization_results
            }
            
        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return {"success": False, "error": str(e)}

    def create_fast_statistics(self, tracks, timestamps):
        """–ë—ã—Å—Ç—Ä–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        if not tracks:
            return self._empty_stats()
        
        speeds = [t['speed_kmh'] for t in tracks]
        vehicle_types = [t['vehicle_type'] for t in tracks]
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        light_count = vehicle_types.count("light")
        heavy_count = vehicle_types.count("heavy")
        congestion_count = sum(1 for s in speeds if s < CONGESTION_SPEED_THRESHOLD)
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑
        start_times = [t['points'][0]['time'] for t in tracks]
        start_time = min(start_times) if start_times else timestamps[0]
        end_time = max(start_times) if start_times else timestamps[-1]
        duration_hours = (end_time - start_time) / 3600
        
        # –ü–æ—á–∞—Å–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑
        hourly_counts = defaultdict(int)
        for track in tracks:
            hour = int(datetime.fromtimestamp(track['points'][0]['time']).strftime('%H'))
            hourly_counts[hour] += 1
        
        peak_hour = max(hourly_counts, key=hourly_counts.get) if hourly_counts else 0
        
        return {
            "total_vehicles": len(tracks),
            "avg_speed_kmh": round(float(np.mean(speeds)), 1) if speeds else 0,
            "congestion_vehicles": congestion_count,
            "congestion_percent": round(congestion_count / len(tracks) * 100, 1) if tracks else 0,
            "peak_hour": f"{peak_hour:02d}:00-{peak_hour+1:02d}:00",
            "traffic_intensity": round(len(tracks) / duration_hours, 1) if duration_hours > 0 else 0,
            "vehicle_types": {"light": light_count, "heavy": heavy_count},
            "processing_time": round(time.time() - self.start_time, 1)
        }

    def _empty_stats(self):
        return {
            "total_vehicles": 0, "avg_speed_kmh": 0, "congestion_vehicles": 0,
            "congestion_percent": 0, "peak_hour": "00:00-01:00", 
            "traffic_intensity": 0, "vehicle_types": {"light": 0, "heavy": 0},
            "processing_time": 0
        }

    def create_fast_visualizations(self, tracks, data_2d, timestamps, stats):
        """–ë—ã—Å—Ç—Ä—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
        visualizations = {}
        
        try:
            visualizations['heatmap'] = self.create_fast_heatmap(tracks, data_2d, timestamps)
            visualizations['infographic'] = self.create_fast_infographic(tracks, stats)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            
        return visualizations

    def create_fast_heatmap(self, tracks, data_2d, timestamps):
        """–ë—ã—Å—Ç—Ä—ã–π heatmap"""
        logger.info("üé® –°–æ–∑–¥–∞–Ω–∏–µ heatmap...")
        
        try:
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            vmax = np.percentile(data_2d, 90)
            im = ax.imshow(data_2d, cmap='hot', aspect='auto', origin='lower',
                          extent=[0, DISTANCE_METERS, 0, len(timestamps) * TIME_STEP / 60],
                          vmax=vmax)
            plt.colorbar(im, ax=ax, label='–ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å')
            
            # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ç—Ä–µ–∫–æ–≤
            colors = {"light": "blue", "heavy": "red"}
            for track in tracks:
                if not track['points']:
                    continue
                    
                positions = [p['position'] for p in track['points']]
                times = [p['time'] for p in track['points']]
                time_minutes = [(t - timestamps[0]) / 60 for t in times]
                
                color = colors.get(track['vehicle_type'], 'blue')
                ax.plot(positions, time_minutes, color=color, linewidth=1.5, alpha=0.7)
            
            ax.set_xlabel("–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–º)")
            ax.set_ylabel("–í—Ä–µ–º—è (–º–∏–Ω)")
            ax.set_title(f"–¢—Ä–∞—Ñ–∏–∫ - {len(tracks)} –¢–°")
            
            # –õ–µ–≥–µ–Ω–¥–∞
            legend_elements = [
                Line2D([0], [0], color='blue', lw=2, label='–õ–µ–≥–∫–æ–≤—ã–µ'),
                Line2D([0], [0], color='red', lw=2, label='–ì—Ä—É–∑–æ–≤—ã–µ')
            ]
            ax.legend(handles=legend_elements, loc='upper right')
            
            plt.tight_layout()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            buffer = BytesIO()
            plt.savefig(buffer, format='png', dpi=80, bbox_inches='tight')
            buffer.seek(0)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            plt.close()
            
            return image_base64
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ heatmap: {e}")
            return ""

    def create_fast_infographic(self, tracks, stats):
        """–ë—ã—Å—Ç—Ä–∞—è –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∞"""
        logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∏...")
        
        try:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
            
            # 1. –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
            metrics = ['–í—Å–µ–≥–æ –¢–°', '–°—Ä. —Å–∫–æ—Ä–æ—Å—Ç—å', '–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç—å']
            values = [
                stats['total_vehicles'],
                stats['avg_speed_kmh'],
                stats['congestion_percent']
            ]
            ax1.bar(metrics, values, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.8)
            ax1.set_title('–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏')
            
            # 2. –¢–∏–ø—ã –¢–°
            vehicle_counts = [stats['vehicle_types']['light'], stats['vehicle_types']['heavy']]
            ax2.pie(vehicle_counts, labels=['–õ–µ–≥–∫–æ–≤—ã–µ', '–ì—Ä—É–∑–æ–≤—ã–µ'], autopct='%1.1f%%', 
                   colors=['#3498db', '#e74c3c'])
            ax2.set_title('–¢–∏–ø—ã –¢–°')
            
            # 3. –°–∫–æ—Ä–æ—Å—Ç–∏
            speeds = [t['speed_kmh'] for t in tracks if t['speed_kmh'] > 0]
            if speeds:
                ax3.hist(speeds, bins=15, color='#9b59b6', alpha=0.7, edgecolor='black')
                ax3.axvline(np.mean(speeds), color='red', linestyle='--', label=f'–°—Ä–µ–¥–Ω—è—è: {np.mean(speeds):.1f}')
                ax3.set_xlabel('–°–∫–æ—Ä–æ—Å—Ç—å (–∫–º/—á)')
                ax3.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
                ax3.legend()
            ax3.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–µ–π')
            
            # 4. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            directions = [t['points'][-1]['position'] > t['points'][0]['position'] for t in tracks]
            dir_counts = [sum(directions), len(directions) - sum(directions)]
            ax4.pie(dir_counts, labels=['–í–ø–µ—Ä–µ–¥', '–ù–∞–∑–∞–¥'], autopct='%1.1f%%',
                   colors=['#2ecc71', '#e74c3c'])
            ax4.set_title('–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è')
            
            plt.tight_layout()
            
            output_path = os.path.join(self.output_dir, "infographic.png")
            plt.savefig(output_path, dpi=80, bbox_inches='tight')
            plt.close()
            
            return output_path
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∏: {e}")
            return None

    def save_results_fast(self, tracks, stats):
        """–ë—ã—Å—Ç—Ä–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        result = {
            "trace_list": tracks,
            "statistics": stats,
            "metadata": {
                "analysis_time": datetime.now().isoformat(),
                "algorithm": "optimized_fast"
            }
        }
        
        with open(os.path.join(self.output_dir, "tracks.json"), "w", encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        # –ü—Ä–æ—Å—Ç–æ–π –æ—Ç—á–µ—Ç
        report = f"""–û–¢–ß–ï–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –¢–†–ê–§–ò–ö–ê
==============================
–í—Å–µ–≥–æ –¢–°: {stats['total_vehicles']}
–õ–µ–≥–∫–æ–≤—ã–µ: {stats['vehicle_types']['light']}
–ì—Ä—É–∑–æ–≤—ã–µ: {stats['vehicle_types']['heavy']}
–°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {stats['avg_speed_kmh']} –∫–º/—á
–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç—å: {stats['congestion_percent']}%
–ü–∏–∫–æ–≤—ã–π —á–∞—Å: {stats['peak_hour']}
–ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å: {stats['traffic_intensity']} –¢–°/—á–∞—Å
=============================="""
        
        with open(os.path.join(self.output_dir, "report.txt"), "w", encoding='utf-8') as f:
            f.write(report)

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    def load_tracks_and_get_time_range(self):
        try:
            with open(os.path.join(self.output_dir, "tracks.json"), "r", encoding='utf-8') as f:
                data = json.load(f)
            tracks = data["trace_list"]
            if not tracks:
                return [], 0, 1
            all_times = [pt["time"] for track in tracks for pt in track["points"]]
            return tracks, min(all_times), max(all_times)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç—Ä–µ–∫–æ–≤: {e}")
            return [], 0, 1

    def create_traffic_heatmap(self, tracks, start_time, end_time, return_base64=False):
        try:
            if self.processed_data is None or self.timestamps is None:
                stats, timestamps = self.load_combined_data()
                data_2d, _ = self.optimized_preprocess(stats)
            else:
                data_2d = self.processed_data
                timestamps = self.timestamps
            
            return self.create_fast_heatmap(tracks, data_2d, timestamps)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è heatmap: {e}")
            return ""

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def generate_all_visualizations(combined_data_path, output_dir):
    visualizer = TrafficVisualization(combined_data_path, output_dir)
    return visualizer.fast_generate_analysis()

def get_visualization_stats(output_dir):
    try:
        with open(os.path.join(output_dir, "tracks.json"), "r", encoding='utf-8') as f:
            data = json.load(f)
        return data["statistics"]
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return {"error": str(e)}